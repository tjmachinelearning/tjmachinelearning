<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Neural Network 4: Hyperparameters and Network Tuning | TJHSST Machine Learning Club</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../../css/demo.css" />
    <link rel="stylesheet" type="text/css" href="../../../css/component.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../favicon-16x16.png">
    <link rel="manifest" href="../../../manifest.json">
    <link rel="mask-icon" href="../../../safari-pinned-tab.svg" color="#5bbad5">
    <meta name="theme-color" content="#ffffff">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-105333430-1', 'auto');
      ga('send', 'pageview');

    </script>
  </head>
  <body>
    <div class="container">
        <h2 style="text-align:center"><a href="../../../"><img src="../../../img/logo.svg" width="200px"></img></a></h2>
         <section class="section section--menu" id="Alonso">
             <span class="link-copy"></span>
             <nav class="menu menu--alonso">
                 <ul class="menu__list">
                     <li class="menu__item"><a href="../../../" class="menu__link">Home</a></li>
                     <li class="menu__item menu__item--current"><a href="../../../schedule/1718" class="menu__link">Lectures</a></li>
                     <li class="menu__item "><a href="../../../rankings" class="menu__link">Rankings</a></li>
                     <li class="menu__item"><a href="../../../resources" class="menu__link">Resources</a></li>
                     <li class="menu__item"><a href="../../../research" class="menu__link">Research</a></li>
                     <li class="menu__item"><a href="../../../competitions/1718" class="menu__link">Competitions</a></li>
                     <li class="menu__line"></li>
                 </ul>
             </nav>
         </section>
    </div>
    <section class="main-content">
        <div class="lecture">
            <h1 style="text-align:center; color:#000">Hyperparameters and Network Tuning</h1>
            <h3 style="text-align:center; color:#000">Mihir Patel</h2>
            <h3 style="text-align:center; color:#000">October 2017</h2>
                <h1 id="introduction">Introduction</h1>
                <p>Neural networks are very powerful tools that are one of the few types of machine learning approaches robust enough to tackle a variety of problems. Their method of learning, while requiring large amounts of data, can face virtually any challenge. However, they also have considerably more hyperparameters to tune compared to previous methods. How do we determine the number of layers? The number of nodes? How do we regularize? The answers to these questions are critical to squeezing out maximum performance.</p>
                <h1 id="network-design">Network Design</h1>
                <p>The first aspect to any structure is determining the size and shape of it. Unfortunately, there really is no good rule on how to do this, or we would have already automated out writing ML algorithms with networks. The obvious ones are the input and output layers, which have to match the specifications of the problem, pairing with the number of features and number of output classes respectively.<br />
                <br />
                As for hidden layers, there really is no good answer. There are some useful tips however. In general, the deeper the network, the more advanced patterns it can learn but the more data and time it requires. Deeper networks are also much more prone to overfitting, as they could potentially just memorize large chunks of the dataset and not learn the general pattern. With deep learning and networks, we assume that our network will not overfit given good hyperparameters and so typically, the deeper the better as long as you have the computational resources for it.<br />
                <br />
                Another good rule is to always narrow down the size of each layer. This helps the network condense information into important higher level features and saves computational resources as you need more lower level features than higher level features. In general, things like this simply just come with experience and trying new combinations if previous methods fail. Usually, the answer is just add more layers if you have the data for it.</p>
                <h1 id="regularization">Regularization</h1>
                <p>As I mentioned earlier, neural networks can be prone to overfitting, especially with deep networks. Overfitting, if you don’t remember, involves a machine learning algorithm learning a sub-pattern or exploit that boosts training accuracy but diminishes knowledge of the overall problem and therefore hurts validation accuracy. The best way to prevent this is to stop training when validation accuracy begins to increase. However, we want to supress this from happening as long as possible so that we can potentially better learn the generalized pattern. The first approach involves modifying the loss function.</p>
                <ol>
                <li><p>L1 regularization: Adds the absolute value of the network weights to the error function. <span class="math display">\[L + \lambda \sum_{i=1}^{k}|w_i|\]</span> In this case, some hyperparameter lambda is multiplied by the absolute value of the weights. L1 has been shown to be better in feature selection for spare feature spaces. This is a bit complicated to demonstrate and involves Laplace transformations. If you’re interested feel free to look it up.</p></li>
                <li><p>L2 regularization: Adds the square of the network weights to the error function. <span class="math display">\[L + \lambda \sum_{i=1}^{k}w_i^2\]</span> This has been shown to be more effective in basically every other scenario.</p></li>
                </ol>
                <p>Both L1 and L2 regularization help prevent overfitting. <span class="math inline">\(\lambda\)</span> controls the extent to which the regularization term matters and is generally around <span class="math inline">\(0.01\)</span>. They work by punishing large weights, preventing large numbers and adding small amounts of noise to help generalize the pattern.</p>
                <h1 id="dropout">Dropout</h1>
                <p>Another method for regularization is dropout. At each backpropagation step, some random percentage of nodes are ignored. See the diagram below to see how this work. This helps prevent highly dependent nodes and ensure each node actually learns something significant. Another way to think about this is stacking several different learners, where each network post-dropout is a different learner. The only difference is that all these networks are stacked into one model. This has been shown to produce several impressive improvements in performance and robustness. Good values of dropout range from <span class="math inline">\(0.2\)</span> to <span class="math inline">\(0.4\)</span> and depend on the layer type, with lower values for things like convolutional layers.</p>
                <p style="text-align:center"><img src="dropout.jpg" alt="image" /></p>
                <h1 id="batch-size">Batch Size</h1>
                <p>Often, large datasets might contain certain wrong data points or the pattern might not be easy to deduce from a single image (see MRI scans). To better adjust weights, we often forward propagate multiple data points before backpropagating on the results to stabilize training, preventing bad weight adjustments from poor data points.<br />
                <br />
                By setting a batch size, we can control how many points we look at before backpropagating. In general, we pick the largest value that our machine supports based on its RAM size. The larger it is, the more stable the network becomes.</p>
                <h1 id="learning-rate">Learning Rate</h1>
                <p>Learning rate represents the <span class="math inline">\(\alpha\)</span> in the backpropagation for updating weights and biases: <span class="math display">\[W_i =  W_i - \alpha \frac{\partial E}{\partial W_i}\]</span> <span class="math display">\[b_i =  b_i - \alpha \delta_i\]</span> This value determines how much we shift the weights at each step. A high learning rate means we might skip over the ideal weights (we can never reach 2.5 if we go from 2 to 3 at a step size of 1) and a low learning rate means it takes forever to train. A low learning rate means that we could also get stuck in a local minimum and not be able to climb out because each side has a higher error for our learning rate step size.</p>
                <p><img src="learningrate.png" alt="image" /></p>
                <p>As this figure shows, a low learning rate (blue) means it goes very slowly. By contrast, a high rate (green) leads to instability and fluctuation. The perfect rate (red) has fast, stable training. In general, we pick a value and then adjust based on speed of training, looking at if the error is slow to adjust or sling-shotting around. This is problem specific and there is no good value, though most libraries provide a default.<br />
                <br />
                In most cases, we use dynamic manipulations of the learning rate. Once we begin to see marginal improvements, we decrease the learning rate so that we can better reach the global minimum. The higher initial rate also speeds up training.</p>
                <h1 id="vanishing-gradient-problem">Vanishing Gradient Problem</h1>
                <p>Earlier I mentioned that adding more layers helps networks better learn patterns. It turns out this isn’t exactly the case. When we reach deeper layers, we see that they begin to train slower and slower, quickly rendering them useless. Why is this the case? Let’s look at how we update the weights. We know we update by:</p>
                <p><span class="math display">\[W_i =  W_i - \alpha \frac{\partial E}{\partial W_i}\]</span> <span class="math display">\[b_i =  b_i - \alpha \delta_i\]</span> and that <span class="math inline">\(\partial E\)</span> with respect to <span class="math inline">\(\partial W_i\)</span> is:</p>
                <p><span class="math display">\[\frac{\partial E}{\partial W_i} = \delta_i x_{i-1}^T\]</span> which means that both weights and biases are dependent on <span class="math inline">\(\delta\)</span> at each layer. The original delta for the first layer is calculated by:</p>
                <p><span class="math display">\[\delta_L = (x_L - y) \odot \sigma&#39;(W_{L}x_{L-1})\]</span></p>
                <p>and every subsequent layer is:</p>
                <p><span class="math display">\[\delta_i = W_{i+1}^T\delta_{i+1} \odot \sigma&#39;(W_ix_{i-1})\]</span></p>
                <p>Here is where things get interesting! We see that every subsequent layer’s delta is the previous delta multiplied by a bunch of things. Now, the thing that we care about is the sigmoid function. We know that once a node converges, the derivative of the sigmoid drops off very quickly. As we see from the graph, once a neuron learns its pattern and when to say 0/1, its derivative drops off. This is good, because we don’t want to mess with that value. However, this means all subsequent layers have lower and lower deltas, learning much slower. This means we can’t make things too deep.</p>
                <p style="text-align:center"><img src="sigmoid.png" alt="image" /></p>
                <h1 id="activation-functions">Activation Functions</h1>
                <p>One way to fix the vanishing gradient problem is by modifying the activation function, which changes the derivative.</p>
                <p style="text-align:center"><img src="activation.jpg" alt="image" /></p>
                <ol>
                <li><p>Sigmoid: <span class="math display">\[\frac{1}{1+e^-x}\]</span> This is the function we’ve been traditionally using and solid in general purpose.</p></li>
                <li><p>Tanh: <span class="math display">\[\frac{e^x-e^{-x}}{e^x+e^{-x}}\]</span> This function has a more complicated derivative so it is a bit slower to compute. However, it prevents flatlining as the values range from -1 to 1 instead of 0 to 1. When you don’t multiply by 0, the values aren’t decreasing as much as much. This leads to better performance. There are a few proofs showing this.</p></li>
                <li><p>Relu: <span class="math display">\[max(0,x)\]</span> This function looks very simple. It ignores everything negative and on the right just takes the positive value. This leads to much worse performance. However, the derivative is very, very easy to compute. It’s 0 on the left of the y-axis and 1 on the right. This means it is very fast and doesn’t have the vanishing gradient problem as much because the derivative is constant. In this way, we hurt the performance of each layer but just stack on a ton more layers.</p></li>
                </ol>
                <h1 id="optimizers">Optimizers</h1>
                <p>We’ve seen that preventing getting stuck in local minima and efficiently updating weights is hard. To account for this, various modifications to the backpropagation algorithm have been propose to produce more efficient weight updating and increased stabilization. We won’t go into how they work because it is decidedly non-trivial and as complex as learning backpropagation in the first place. However, there are a few key features that we will highlight.</p>
                <ol>
                <li><p>Learning Rate: We already covered this! Go back a few pages.</p></li>
                <li><p>Momentum: Momentum is a trick to prevent getting stuck in local minima. At each update, we factor in the change that we did the previous step, maintaining larger strides if our previous strides were big and vice-versa.</p></li>
                <li><p>Decay: At each step, the weights are all multiplied by a constant less than one. This prevents exploding values just like L1 and L2 regularization.</p></li>
                <li><p>Epsilon: At each step, some fuzz / noise is applied to the weights and biases, helping increase regularization.</p></li>
                <li><p>Lots of other variables: Read the documentation for specific algorithms.</p></li>
                </ol>
                <p>Here are a brief sample of some optimizers that are most commonly used. There are lots of other ones, though in general they are very ad-hoc or outdated.</p>
                <ol>
                <li><p>(Stochastic/Mini-batch/Batch) Gradient Descent: Normal updates! Basically the run-of-the-mill algorithm that involves the least amount of computation.</p></li>
                <li><p>RMSProp: Divides learning rate for a given weight by a running average the magnitude of recent gradients. Useful in RNNs.</p></li>
                <li><p>Adam: Also adapts learning rates for given weights. Similar to RMSProp, but also includes momentum-like functionality. In general, this is the best one to use.</p></li>
                </ol>

            </div>
        <p><a href="../../../schedule/1718">&larr; Back to lectures</a></p>
</section>
<footer class="site-footer" style="background-image:linear-gradient(120deg, #272d39, #272d39);">
    <section class="main-content">
        <table style="border: 0px; width=100%;">
            <tr style="border: 0px">
                <td style="border:0px">
                    <a href="https://tjmachinelearning.com"><img src="../../../img/logo_alt.svg" width="150em" style="vertical-align:middle; margin-right:1em"></img></a>
                </td>
                <td style="border:0px; text-align: left">
                    <p style="color:#fff">TJ Machine Learning Club<br>6560 Braddock Rd, Alexandria, VA 22312<br><code style="background-color:#000"><a href="https://github.com/nikhilsardana/tjmachinelearning">Open-source</a></code></p>
                </td>
          </tr>
        </table>
    </section>
</footer>
  </body>
</html>
