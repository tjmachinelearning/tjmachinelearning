<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Fully Convolutional Networks | TJHSST Machine Learning Club</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../../stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../../css/demo.css" />
    <link rel="stylesheet" type="text/css" href="../../../css/component.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../favicon-16x16.png">
    <link rel="manifest" href="../../../manifest.json">
    <link rel="mask-icon" href="../../../safari-pinned-tab.svg" color="#5bbad5">
    <meta name="theme-color" content="#ffffff">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-105333430-1', 'auto');
      ga('send', 'pageview');

    </script>
  </head>
  <body>
    <div class="container">
        <h2 style="text-align:center"><a href="../../../"><img src="../../../img/logo.svg" width="200px"></img></a></h2>
         <section class="section section--menu" id="Alonso">
             <span class="link-copy"></span>
             <nav class="menu menu--alonso">
                 <ul class="menu__list">
                     <li class="menu__item"><a href="../../../" class="menu__link">Home</a></li>
                     <li class="menu__item menu__item--current"><a href="../../../schedule/1718" class="menu__link">Lectures</a></li>
                     <li class="menu__item "><a href="../../../rankings" class="menu__link">Rankings</a></li>
                     <li class="menu__item"><a href="../../../resources" class="menu__link">Resources</a></li>
                     <li class="menu__item"><a href="../../../research" class="menu__link">Research</a></li>
                     <li class="menu__item"><a href="../../../competitions/1718" class="menu__link">Competitions</a></li>
                     <li class="menu__line"></li>
                 </ul>
             </nav>
         </section>
    </div>
    <section class="main-content">
        <div class="lecture">
            <h1 style="text-align:center; color:#000">Fully Convolutional Networks</h1>
            <h3 style="text-align:center; color:#000">Nikhil Sardana</h2>
            <h3 style="text-align:center; color:#000">December 2017</h2>
                <h1 id="introduction">Introduction</h1>
                <p>A traditional convolutional network has multiple convolutional layers, each followed by pooling layer(s), and a few fully connected layers at the end. These standard CNNs are used primarily for image classification. This lecture covers Fully Convolutional Networks (FCNs), which differ in that they do not contain any fully connected layers. This lecture is intended for readers with understanding of traditional CNNs. We will explore the structure and purpose of FCNs, along with their application to semantic segmentation.</p>
                <h1 id="semantic-segmentation">Semantic Segmentation</h1>
                <p>We’ve previously covered classification (without localization). Later lectures will cover object detection and instance segmentation. The main difference between semantic segmentation and instance segmentation is that we make no distinction between the instances of a particular class in semantic segmentation. We simply wish to classify every single pixel.</p>
                <p style="text-align:center"><img src="tasks.PNG" alt="image" /></p>
                <p>How can we adapt convolutional networks to classify every single pixel? Clearly, we could take a small crop of the original image centered around a pixel, use the central pixel’s class as the ground truth of the crop, and run the crop through a CNN. However, we would need a crop for every single pixel in an image, and this would be hopelessly slow. What if we could classify every single pixel at once? Enter Fully Convolutional Networks.</p>
                <h1 id="network-architecture">Network Architecture</h1>
                <p>What if we just remove the pooling layers and fully connected layers from a convolutional network? Then, at the end, we could have a layer with depth <span class="math inline">\(C\)</span>, where <span class="math inline">\(C\)</span> is the number of classes. We can choose a filter size and stride length to maintain our original image width <span class="math inline">\(W\)</span> and height <span class="math inline">\(H\)</span> throughout the entire network, so we could simply make our loss function a sum of the cross-entropy loss for each pixel (remember, we are essentially performing classification for each pixel). Refer to the diagram below for a visual representation of this network.</p>
                <p style="text-align:center"><img src="fullyconv.PNG" alt="image" /></p>
                <p>Obviously, this network will run far quicker than simply classifying each pixel individually. However, it is still too computationally expensive. Using the original input image size throughout the entire network would be extremely expensive (especially for deep networks). Thus, we need a way to downsample the image (just like in a standard convolutional network), and then, upsample the layers back to the original image size.</p>
                <p style="text-align:center"><img src="structure.png" alt="image" /></p>
                <p>The above diagram shows a fully convolutional network. The first half is identical to the Convolutional/Pooling layer structure that makes up most of traditional CNN architecture. Through pooling and strided convolutions, we reduce the size of each layer, reducing computation. However, instead of having fully connected layers (which are at the end of normal CNNs), we have <span class="math inline">\(1\times1\)</span> convolutional layers.</p>
                <h2 id="times1-convolutions">1<span class="math inline">\(\times\)</span>1 Convolutions</h2>
                <p>It is important to realize that <span class="math inline">\(1\times1\)</span> convolutional layers are actually the same thing as fully connected layers. Think about it. In the traditional CNN below, how exactly do we get from the <span class="math inline">\(5\times5\)</span> layer to the first fully connected layer?</p>
                <p style="text-align:center"><img src="standardconv.png" alt="image" /></p>
                <p>It’s simple! The first fully connected layer is simply a convolutional layer with a <span class="math inline">\(5\times5\)</span> kernel. If it’s still unclear, here’s an example with numbers: <span class="math display">\[\begin{bmatrix}
                  1 &amp; 2 &amp; 3 &amp; 1 &amp; 3\\
                  4 &amp; 5 &amp; 6 &amp; 1 &amp; 2\\
                  7 &amp; 8 &amp; 9 &amp; 1 &amp; 4\\
                  2 &amp; 1 &amp; 3 &amp; 5 &amp; 4\\
                  2 &amp; 4 &amp; 2 &amp; 1 &amp; 1\\
                \end{bmatrix}
                *
                \begin{bmatrix}
                  2 &amp; 2 &amp; 2 &amp; 2 &amp; 2\\
                  2 &amp; 2 &amp; 2 &amp; 2 &amp; 2\\
                  2 &amp; 2 &amp; 2 &amp; 2 &amp; 2\\
                  2 &amp; 2 &amp; 2 &amp; 2 &amp; 2\\
                  2 &amp; 2 &amp; 2 &amp; 2 &amp; 2\\

                \end{bmatrix}
                =
                \begin{bmatrix}
                  164\\
                \end{bmatrix}\]</span></p>
                <p>That single number, <span class="math inline">\(164\)</span>, would become the value of a single neuron in the first fully connected layer. For each <span class="math inline">\(5\times5\)</span> feature map, we have a <span class="math inline">\(5\times5\)</span> kernel, and generate a neuron in the first fully connected layer. You can think of all the other fully connected layers as just stacks of <span class="math inline">\(1\times1\)</span> convolutions (with <span class="math inline">\(1\times1\)</span> kernels, obviously).</p>
                <p>Of course, you ask, if fully connected layers are simply <span class="math inline">\(1\times1\)</span> convolutional layers, then why don’t all CNNs just use <span class="math inline">\(1\times1\)</span> convolutional layers at the end, instead of fully connected layers?</p>
                <p>Simply put, newer networks do. Sometimes, older networks like VGG16 have their fully connected layers reimplemented as conv layers (see SSD). Any MLP can be reimplemented as a CNN. For example, a standard NN with <span class="math inline">\(n\)</span> inputs is also a convolutional network with an input of a single pixel, and <span class="math inline">\(n\)</span> input channels.</p>
                <p>There is, however, one very important difference between a fully convolutional network and a standard CNN. Consider the standard convolutional network above. Note how a fully connected layer expects an input of a particular size. This restricts our input image to a fixed size. A fully convolutional network has no such issues. Since no fully connected layers exist, our input can be of any size.</p>
                <h2 id="unpooling">Unpooling</h2>
                <p>We now understand the first half of the network (including the <span class="math inline">\(1\times1\)</span> convolutional layers). The question remains: How do we increase layer size to reach the dimensions of the original input? One way we can upsample is by unpooling.</p>
                <p style="text-align:center"><img src="unpooling1.PNG" alt="image" /></p>
                <p>There are multiple approaches to unpooling. One approach is “Nearest Neighbor&quot;, we simply repeat every element. “Bed of Nails&quot; unpooling simply places the value in a particular position in the output, filling the rest with zeros. The above example places the input values in the upper left corner.</p>
                <p style="text-align:center"><img src="maxunpooling.png" alt="image" /></p>
                <p>Max Unpooling is a smarter “bed of nails&quot; method. Rather than a predetermined, fixed location for the “nails&quot;, we use the position of the maximum elements from the corresponding max pooling layer earlier in the network. This works because Fully Convolutional Networks are often symmetric, and each convolutional and pooling layer corresponds to a transposed convolution (also called <em>deconvolution</em>) and unpooling layer.</p>
                <h2 id="transposed-convolution">Transposed Convolution</h2>
                <p>Strided convolutions allow us to decrease layer size in a learnable fashion. Pooling is a fixed function, however, we learn the weights of a convolutional layer, and thus a strided convolution is more powerful than a pooling layer. Strided convolutions are to pooling layers what transposed convolutions are to unpooling layers.</p>
                <p>You will often hear transposed convolution referred to as <em>deconvolution</em>. Deconvolution suggests the opposite of convolution, however, a transposed convolution is simply a normal convolution operation, albeit with special padding.</p>
                <p style="text-align:center">
                <img src="normal.gif" alt="Normal convolution."/>
                <img src="transposed.gif" alt="Transposed convolution."/></p>
                <p>In the figure above left, we get from a <span class="math inline">\(5\times5\)</span> layer (blue) to a <span class="math inline">\(2\times2\)</span> layer (green) by performing a convolution with filter size <span class="math inline">\(3\)</span>, and stride <span class="math inline">\(2\)</span>. With some fancy padding in the transposed convolution, we achieve the opposite: <span class="math inline">\(2\times2\)</span> to <span class="math inline">\(5\times5\)</span>. Thus, transpose convolutions allow us to increase our layer size in a learnable fashion, since we can change the weights through backpropagation.</p>
                <p>We can clearly see that we will not end up with our original <span class="math inline">\(5\times5\)</span> values if we perform the normal convolution, and then the transpose convolution. The transpose convolution is <em>not</em> the inverse of a convolution, and thus deconvolution is a terrible name for the operation.</p>
                <p>Now we have covered both ends of the Fully Convolutional Network. We begin with a standard CNN, and use strided convolutions and pooling to downsample from the original image. Then, we upsample using unpooling and transposed convolutions. Finally, we end up with a <span class="math inline">\(C\times H \times W\)</span> layer, where <span class="math inline">\(C\)</span> is the number of classes, and <span class="math inline">\(H\)</span> and <span class="math inline">\(W\)</span> are the original image height and width, respectively. Thus, we get a prediction for each pixel, and perform semantic segmentation.</p>
                <h1 id="skip-connections">Skip Connections</h1>
                <p>“Fully Convolutional Networks for Semantic Segmentation&quot; by Long et al. introduced the idea of skip connections into FCNs to improve segmentation accuracy. (It also popularized FCNs as a method for semantic segmentation).</p>
                <p>Upsampling using transposed convolutions or unpooling loses information, and thus produces coarse segmentation. Skip connections allow us to produce finer segmentation by using layers with finer information.</p>
                <p>Skip connections combine the coarse final layer with finer, earlier layers to provide local predictions that “respect&quot; global positions. Refer to the figure below for a diagram of the skip connection architecture.</p>
                <p>To create FCN-16s, the authors added a <span class="math inline">\(1\times1\)</span> convolution to pool4 to create class predictions, and fused these predictions with the predictions computed by conv7 with a <span class="math inline">\(2\times\)</span> upsampling layer. For FCN-8s, they added a <span class="math inline">\(2\times\)</span> upsampling layer to this output, and fused it with the predictions from a <span class="math inline">\(1\times1\)</span> convolution added to pool3.</p>
                <p style="text-align:center"><img src="skipconnections.PNG" alt="image" /></p>
                <p>The figure below left shows that FCN-16s provides much finer segmentation than the standard FCN-32s, and FCN-8s even finer segmentation (much closer to ground truth). The accuracy table below right quantifies the segmentation improvement from skip connections.</p>
                <p style="text-align:center"><img src="fcnresults.PNG" alt="image" style="width:80.0%" /></p>
                <p style="text-align:center"><img src="fcntable.PNG" alt="image" style="width:60.0%" /></p>
                <h1 id="further-work-in-semantic-segmentation">Further Work in Semantic Segmentation</h1>
                <p>It should be noted that to max unpooling with saved indices we cover in Section 3.2 was not introduced in the FCN paper above, but rather a later paper called SegNet.</p>
                <p style="text-align:center"><img src="segnet.PNG" alt="image" /></p>
                <p style="text-align:center"><img src="segnetvsfcn.PNG" alt="image" /></p>
                <p>Not unsurprisingly, SegNet performed better than standard FCNs with skip connections. Nevertheless, SegNet has been surpassed numerous times by newer papers using dialated convolutions, spatial pyramid pooling, and residual connections. We will cover these in a later lecture dedicated to semantic segmentation.</p>
                <p style="text-align:center"><img src="segnetresults.PNG" alt="image"/></p>
                <h1 id="acknowledgements">Acknowledgements</h1>
                <p>None of the diagrams were created by me.</p>
                <ul>
                <li><p><a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf/">“Fully Convolutional Networks for Semantic Segmentation&quot; paper</a></p></li>
                <li><p><a href="https://arxiv.org/pdf/1511.00561.pdf">“SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation&quot; paper</a></p></li>
                <li><p><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf">Many diagrams from these CS231n slides</a></p></li>
                <li><p><a href="https://github.com/vdumoulin/conv_arithmetic">CNN gifs</a></p></li>
                <li><p><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf">FCN diagram from this paper</a></p></li>
                <li><p><a href="   https://www.kernix.com/doc/data/cnn.png">CNN diagram</a></p></li>
                </ul>

            </div>
        <p><a href="../../../schedule/1718">&larr; Back to lectures</a></p>
</section>
<footer class="site-footer" style="background-image:linear-gradient(120deg, #272d39, #272d39);">
    <section class="main-content">
        <table style="border: 0px; width=100%;">
            <tr style="border: 0px">
                <td style="border:0px">
                    <a href="https://tjmachinelearning.com"><img src="../../../img/logo_alt.svg" width="150em" style="vertical-align:middle; margin-right:1em"></img></a>
                </td>
                <td style="border:0px; text-align: left">
                    <p style="color:#fff">TJ Machine Learning Club<br>6560 Braddock Rd, Alexandria, VA 22312<br><code style="background-color:#000"><a href="https://github.com/nikhilsardana/tjmachinelearning">Open-source</a></code></p>
                </td>
          </tr>
        </table>
    </section>
</footer>
  </body>
</html>
