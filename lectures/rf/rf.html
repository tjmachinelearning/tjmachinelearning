<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Random Forests | TJHSST Machine Learning Club</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../css/demo.css" />
    <link rel="stylesheet" type="text/css" href="../../css/component.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../manifest.json">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#5bbad5">
    <meta name="theme-color" content="#ffffff">
  </head>
  <body>
    <div class="container">
        <h2 style="text-align:center">TJHSST Machine Learning</h2>
         <section class="section section--menu" id="Alonso">
             <span class="link-copy"></span>
             <nav class="menu menu--alonso">
                 <ul class="menu__list">
                     <li class="menu__item"><a href="../../index.html" class="menu__link">Home</a></li>
                     <li class="menu__item menu__item--current"><a href="../../schedule.html" class="menu__link">Lectures</a></li>
                     <li class="menu__item "><a href="../../rankings.html" class="menu__link">Rankings</a></li>
                     <li class="menu__item"><a href="../../resources.html" class="menu__link">Resources</a></li>
                     <li class="menu__item"><a href="../../projects.html" class="menu__link">Projects</a></li>
                     <li class="menu__item"><a href="../../submit.html" class="menu__link">Updates</a></li>
                     <li class="menu__line"></li>
                 </ul>
             </nav>
         </section>
    </div>
    <section class="main-content">
        <h2 style="text-align:center; color:#000">Random Forests</h2>
        <h3 style="text-align:center; color:#000">Sylesh Suresh</h2>
        <h3 style="text-align:center; color:#000">November 2017</h2>

          <h1 id="introduction">Introduction</h1>
          <p>Random forests are an ensemble of decision trees, combining many weak learners to build a robust, strong learner that generalizes better to unseen data than the individual weak learners.</p>
          <h1 id="building-a-random-forest">Building a Random Forest</h1>
          <ol>
          <li><p>Randomly choose <span class="math inline">\(n\)</span> samples from the training set with replacement (i.e. draw a bootstrap sample of size <span class="math inline">\(n\)</span>).</p></li>
          <li><p>Build a decision tree using the bootstrap sample. At each node:</p>
          <ol>
          <li><p>Randomly select <span class="math inline">\(d\)</span> features without replacement.</p></li>
          <li><p>Split the node using the feature among the <span class="math inline">\(d\)</span> features previously selected that provides the best split by maximizing the information gain.</p></li>
          </ol></li>
          <li><p>Repeat steps 1 to 2 <span class="math inline">\(k\)</span> times</p></li>
          <li><p>Aggregate the prediction by each tree by majority vote to assign the final class label.</p></li>
          </ol>
          <p>Note that when building the decision trees, instead of evaluating all features to find the best split, we select the best feature among only a randomly chosen subset of those features. This creates more diversity among the trees, helping to prevent overfitting. Because of these overfitting-preventative measures, it is not necessary to prune the trees. Individually, each decision tree would perform poorly, but the aggregate of many of these trees leads to a model with a much higher performance.</p>
          <h2 id="hyperparameters">Hyperparameters</h2>
          <p>The most important hyperparameter to be optimized here is <span class="math inline">\(k\)</span>, the number of decision trees the random forest uses. Although more trees increase the performance of the classifier as a whole, they also increase the computational expense. The hyperparameters <span class="math inline">\(n\)</span> and <span class="math inline">\(d\)</span> can also be optimized. The bootstrap sample size <span class="math inline">\(n\)</span> correlates with the degree of overfitting; larger values of n decrease the randomness, increasing the likelihood of overfitting while smaller values of n increase the randomness at the cost of the model’s performance. A good balance is to set <span class="math inline">\(n\)</span> to the size of the training set. Similarly, the size of the feature subset <span class="math inline">\(d\)</span> must be less than the number of features in order to promote diversity among the trees, but must be large enough to not reduce the model’s performance. A common convention is to set <span class="math inline">\(d\)</span> to the square root of the number of features.</p>
          <h2 id="extra-trees">Extra-Trees</h2>
          <p>Instead of searching for the best threshold at each split when building each decision tree, another option is to randomly choose thresholds for each feature of the chosen feature subset and choose the best one from these. This further prevents overfitting at the cost of performance. A random forest built this way is called an Extremely Randomized Trees, or Extra-Trees.</p>
          <h1 id="feature-importance">Feature Importance</h1>
          <p>Random Forests can be helpful in ranking and evaluating feature importance. In an individual decision tree, intuitively, the most important features are split near the root of the tree while less important features are either split near the leaf nodes or are not split at all. In other words, the depth of a feature in a decision tree gives a measure of the importance of that feature. In a random forest, one can take advantage of this property by taking the average depth of a feature across all the individual trees to measure its importance. Here is an example of a random forest ranking the importance of pixels in a face recognition problem where the brighter pixels are more important:</p>
          <p style="text-align:center"><img src="featureimportance.jpg" alt="image" /></p>

        <p><a href="../../schedule.html">&larr; Back to lectures</a>
    </section>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-105333430-1', 'auto');
      ga('send', 'pageview');

    </script>
  </body>
</html>
