<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Support Vector Machines | TJHSST Machine Learning Club</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../css/demo.css" />
    <link rel="stylesheet" type="text/css" href="../../css/component.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../manifest.json">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#5bbad5">
    <meta name="theme-color" content="#ffffff">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-105333430-1', 'auto');
      ga('send', 'pageview');

    </script>
  </head>
  <body>
    <div class="container">
        <h2 style="text-align:center"><a href="../../index.html"><img src="../../img/logo.svg" width="200px"></img></a></h2>
         <section class="section section--menu" id="Alonso">
             <span class="link-copy"></span>
             <nav class="menu menu--alonso">
                 <ul class="menu__list">
                     <li class="menu__item"><a href="../../index.html" class="menu__link">Home</a></li>
                     <li class="menu__item menu__item--current"><a href="../../schedule.html" class="menu__link">Lectures</a></li>
                     <li class="menu__item "><a href="../../rankings.html" class="menu__link">Rankings</a></li>
                     <li class="menu__item"><a href="../../resources.html" class="menu__link">Resources</a></li>
                     <li class="menu__item"><a href="../../projects.html" class="menu__link">Projects</a></li>
                     <li class="menu__item"><a href="../../submit.html" class="menu__link">Updates</a></li>
                     <li class="menu__line"></li>
                 </ul>
             </nav>
         </section>
    </div>
    <section class="main-content">
        <div class="lecture">
            <h1 style="text-align:center; color:#000">Support Vector Machines</h1>
            <h3 style="text-align:center; color:#000">Sylesh Suresh</h2>
            <h3 style="text-align:center; color:#000">October 2017</h2>

              <h1 id="introduction">Introduction</h1>
              <p>Support Vector Machines (SVMs) are one of the most popular supervised learning models today, able to perform both linear and nonlinear classification.</p>
              <h1 id="linear-classification">Linear Classification</h1>
              <p>The idea behind SVMs is to maximize the margin, the distance between the hyperplane (decision boundary) and the samples nearest to this hyperplane, called support vectors.</p>
              <div class="figure">
              <img src="svm.jpg" alt="Support Vector Machine" />
              <p class="caption">Support Vector Machine<span data-label="fig:svm"></span></p>
              </div>
              <p>The decision boundaries to the left separate the training data correctly but would not generalize well to unseen data, being too close to the training samples (i.e. having a small margin). On the other hand, the decision boundary to the right marked by the dashed line separates the training data and generalizes well to unseen data, having a large margin. Maximization of the margin allows for the least generalization error.</p>
              <p><span class="math inline">\(\mathbf{w}\)</span> is defined as a vector normal to the decision boundary. The positive hyperplane is defined as <span class="math display">\[\mathbf{w \cdot x_{pos}} + w_0 = 1\]</span> while the negative hyperplane is: <span class="math display">\[\mathbf{w \cdot x_{neg}} + w_0 = -1\]</span></p>
              <p>We can combine these equations by subtracting the second equation from the first: <span class="math display">\[\label{eq:hyperplanes}
              \mathbf{w}{(x_{pos} - x_{neg})} = 2\]</span></p>
              <p>To calculate the margin, first, let us take the difference between a positive support vector and a negative support vector.</p>
              <p><span class="math display">\[x_{pos} - x_{neg}\]</span></p>
              <p>Then, we need to multiply this by a unit vector perpendicular to the hyperplanes. We earlier defined <span class="math inline">\(\mathbf{w}\)</span> to be normal to the hyperplanes, so <span class="math inline">\(\frac{\mathbf{w}}{||\mathbf{w}||}\)</span> serves this purpose:</p>
              <p><span class="math display">\[\frac{\mathbf{w}(x_{pos} - x_{neg})}{||\mathbf{w}||}\]</span></p>
              <p>Using Â [eq:hyperplanes], we arrive at:</p>
              <p><span class="math display">\[\frac{\mathbf{w}(x_{pos} - x_{neg})}{||\mathbf{w}||} = \frac{2}{||\mathbf{w}||}\]</span></p>
              <p>We must maximize <span class="math inline">\(\frac{2}{||\mathbf{w}||}\)</span> to maximize the margin. For mathematical convenience, we can minimize <span class="math inline">\(\frac{1}{2}{||\mathbf{w}||^2}\)</span> to achieve the same effect. The constraint for this optimization problem is that the samples are actually classified correctly:</p>
              <p><span class="math display">\[\mathbf{w \cdot x_{i}} + w_0 \geq 1 \text{ if $y_i = 1$}\]</span></p>
              <p><span class="math display">\[\mathbf{w \cdot x_{i}} + w_0 &lt; -1 \text{ if $y_i = -1$}\]</span></p>
              <p>where <span class="math inline">\(x_i\)</span> is a particular sample and <span class="math inline">\(y_i\)</span> is the class of the sample. More compactly:</p>
              <p><span class="math display">\[y_i(w_0 + \mathbf{w \cdot x_i}) \geq 1\]</span></p>
              <p>After maximizing the margin, our decision rule is:</p>
              <p><span class="math display">\[y_i = sign(\mathbf{w \cdot x_i} + w_0)\]</span></p>
              <p>That is, points to the left of the decision boundary are classified as negative while points to the right are classified as positive.</p>
              <h1 id="nonlinear-classification-using-kernels">Nonlinear Classification using Kernels</h1>
              <p>In the real world, data is usually not linearly separable, meaning that the support vector machine as cannot accurately separate the data. However, we can project the data onto a higher dimensional space where the data is linearly separable using a mapping function <span class="math inline">\(\phi{(\cdot)}\)</span> For example: <span class="math display">\[\phi{(x_1, x_2)} = (z_1, z_2, z_3) = (x_1, x_2, x_1^2 + x_2^2)\]</span> Using this mapping function allows us to separate the two classes below (indicated by red and blue) with a linear hyperplane. We can then project this back into two-dimensional space where the decision boundary becomes nonlinear.</p>
              <div class="figure">
              <img src="dimensions.jpg" alt="Projecting to higher space" />
              <p class="caption">Projecting to higher space<span data-label="fig:mapping"></span></p>
              </div>
              <p>The problem, however, with this approach is its efficiency. When solving the optimization problem of maximizing the margin, the pair-wise dot products of different training samples <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_j}\)</span> must be calculated, a very computationally expensive process in high-dimensional space. To solve this, we can use the kernel trick; we can use kernel functions to implicitly calculate the dot product of <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(\mathbf{x_j}\)</span> without explicitly projecting them into higher dimensional space.</p>
              <p>One of the most popular kernel functions is the Radial Basis Function kernel (RBF kernel) or Gaussian kernel:</p>
              <p><span class="math display">\[k(\mathbf{x_i}, \mathbf{x_j}) = \exp{(-\gamma||\mathbf{x_i}-\mathbf{x_j}||^2})\]</span></p>
              <p><span class="math inline">\(\gamma\)</span> is a free parameter that can be optimized.</p>
          </div>
        <p><a href="../../schedule.html">&larr; Back to lectures</a>
    </section>
  </body>
</html>
